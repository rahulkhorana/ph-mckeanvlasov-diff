{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a4f3c5c5",
      "metadata": {
        "id": "a4f3c5c5"
      },
      "source": [
        "# McKean–Vlasov 3D Diffusion — Colab Runner (GPU + Drive + Private Git)\n",
        "\n",
        "**What this does**\n",
        "1. Mounts Google Drive (datasets in, artifacts out)\n",
        "2. Installs JAX (CUDA), Flax, Optax, Torch (CPU-only for `.pt`)\n",
        "3. Clones your **private** GitHub repo/branch without printing your token\n",
        "4. Runs `main.py` on GPU, saving checkpoints/samples to Drive\n",
        "5. Visualizes latest generated samples (`.npy`) as 3D MPL landscapes\n",
        "\n",
        "**Before you run**\n",
        "- Put your dataset `.pt` in Drive, e.g. `/MyDrive/datasets/unified_topological_data_v6_semifast.pt`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2c81b926",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c81b926",
        "outputId": "dfc73aad-4354-410d-bd4b-806c6657d84b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IN_COLAB: True\n",
            "Mounted at /content/drive\n",
            "Tue Aug 19 16:34:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   34C    P8             11W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "JAX devices: [CudaDevice(id=0)]\n",
            "Backend: gpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2785876187.py:23: DeprecationWarning: jax.lib.xla_bridge.get_backend is deprecated; use jax.extend.backend.get_backend.\n",
            "  print(\"Backend:\", jax.lib.xla_bridge.get_backend().platform)\n"
          ]
        }
      ],
      "source": [
        "# Colab / Drive / GPU setup + deps\n",
        "import os, sys, subprocess, json, time, textwrap, glob\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "print(\"IN_COLAB:\", IN_COLAB)\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    # Show GPU\n",
        "    try:\n",
        "        print(subprocess.check_output([\"nvidia-smi\"], text=True))\n",
        "    except Exception as e:\n",
        "        print(\"No NVIDIA GPU visible:\", e)\n",
        "\n",
        "import jax\n",
        "print(\"JAX devices:\", jax.devices())\n",
        "print(\"Backend:\", jax.lib.xla_bridge.get_backend().platform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "11c417db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c417db",
        "outputId": "1335df04-9251-4513-9f9b-cf5ffd3555be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CWD: /content/drive/MyDrive/ph-mckeanvlasov-diff/src/mckean-vlasov\n",
            "mckean-vlasov/\n",
            "  .DS_Store\n",
            "  __pycache__/\n",
            "    dataloader.cpython-311.pyc\n",
            "    dataloader.cpython-312.pyc\n",
            "    losses_steps.cpython-311.pyc\n",
            "    losses_steps.cpython-312.pyc\n",
            "    models.cpython-311.pyc\n",
            "    models.cpython-312.pyc\n",
            "    sampling.cpython-311.pyc\n",
            "    sampling.cpython-312.pyc\n",
            "  dataloader.py\n",
            "  losses_steps.py\n",
            "  main.py\n",
            "  models.py\n",
            "  read_back.py\n",
            "  run.ipynb\n",
            "  runs/\n",
            "    mv_sde/\n",
            "  sampling.py\n",
            "Found .pt candidates: ['/content/drive/MyDrive/ph-mckeanvlasov-diff/datasets/unified_topological_data_v6_semifast.pt']\n"
          ]
        }
      ],
      "source": [
        "# Point this to the folder in Drive that contains your code:\n",
        "# The folder should have: dataloader.py, models.py, losses_steps.py, sampling.py, main.py, etc.\n",
        "# Example: /content/drive/MyDrive/mckean-vlasov\n",
        "REPO_DIR = Path(\"/content/drive/MyDrive/ph-mckeanvlasov-diff/src/mckean-vlasov\").resolve()\n",
        "\n",
        "assert REPO_DIR.exists(), f\"Repo dir not found: {REPO_DIR}\"\n",
        "os.chdir(REPO_DIR)\n",
        "print(\"CWD:\", Path.cwd())\n",
        "\n",
        "# Quick tree for sanity\n",
        "def tree(path, max_levels=2, prefix=\"\"):\n",
        "    path = Path(path)\n",
        "    print(prefix + path.name + \"/\")\n",
        "    if max_levels <= 0:\n",
        "        return\n",
        "    for p in sorted(path.iterdir()):\n",
        "        if p.is_dir():\n",
        "            tree(p, max_levels-1, prefix + \"  \")\n",
        "        else:\n",
        "            print(prefix + \"  \" + p.name)\n",
        "\n",
        "tree(REPO_DIR, max_levels=2)\n",
        "\n",
        "# Verify required files exist\n",
        "required = [\"dataloader.py\",\"models.py\",\"losses_steps.py\",\"sampling.py\",\"main.py\"]\n",
        "missing = [f for f in required if not (REPO_DIR/f).exists()]\n",
        "assert not missing, f\"Missing files: {missing}\"\n",
        "\n",
        "# Locate dataset .pt (edit if needed)\n",
        "CANDIDATES = glob.glob(\"/content/drive/MyDrive/ph-mckeanvlasov-diff/**/unified_topological_data*.pt\", recursive=True)\n",
        "print(\"Found .pt candidates:\", CANDIDATES[:3])\n",
        "if not CANDIDATES:\n",
        "    print(\">>> If you don't see your dataset, set DATA_PT manually in next cell.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Env setup"
      ],
      "metadata": {
        "id": "BQMraSRc0CCh"
      },
      "id": "BQMraSRc0CCh"
    },
    {
      "cell_type": "code",
      "source": [
        "%pip -q install gudhi==3.11.0 multipers==2.3.2 geomstats==2.8.0 POT==0.9.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpyxtD4TJe6B",
        "outputId": "0fe49b05-fcb3-4753-8171-619dd29805b1"
      },
      "id": "WpyxtD4TJe6B",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/552.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.2/552.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pykeops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keopscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: KeOps forced to CPU (avoid NVCC builds on Colab)\n",
        "%pip -q install pykeops==2.2.3\n",
        "import os\n",
        "os.environ[\"KEOPS_VERBOSE\"]=\"0\"\n",
        "os.environ[\"PYKEOPS_FORCE_BUILD\"]=\"1\"\n",
        "os.environ[\"USE_CUDA\"]=\"0\"\n",
        "print(\"PyKeOps set to CPU mode.\")\n",
        "\n",
        "# Sanity check\n",
        "import os\n",
        "os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"]=\"platform\"  # grow-as-needed GPU memory\n",
        "import jax, jax.numpy as jnp, numpy as np, flax, optax, ml_dtypes\n",
        "print(\"JAX:\", jax.__version__, \"| Devices:\", jax.devices())\n",
        "x = jnp.ones((2048,2048))\n",
        "print(\"Matmul:\", (x@x).block_until_ready().shape)\n",
        "import torch\n",
        "print(\"Torch:\", torch.__version__, \"CUDA avail:\", torch.cuda.is_available())\n",
        "print(\"NumPy:\", np.__version__, \"Flax:\", flax.__version__, \"Optax:\", optax.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgAcPbPBItzZ",
        "outputId": "4e3323f5-4d4b-4cc7-df98-79f29a3e951f"
      },
      "id": "mgAcPbPBItzZ",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/92.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pykeops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keopscore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "PyKeOps set to CPU mode.\n",
            "JAX: 0.5.3 | Devices: [CudaDevice(id=0)]\n",
            "Matmul: (2048, 2048)\n",
            "Torch: 2.6.0+cu124 CUDA avail: True\n",
            "NumPy: 2.0.2 Flax: 0.10.6 Optax: 0.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
        "!export XLA_PYTHON_CLIENT_PREALLOCATE=false\n",
        "!export XLA_FLAGS=\"--xla_gpu_autotune_level=2 --xla_gpu_enable_triton=false\""
      ],
      "metadata": {
        "id": "pKPu0XUG8OR-"
      },
      "id": "pKPu0XUG8OR-",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "96dd91c4",
      "metadata": {
        "id": "96dd91c4"
      },
      "source": [
        "## Repo / Paths\n",
        "Fill in your repo/user/branch and paths. Artifacts will go to Drive under `OUTDIR`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae62e0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bae62e0b",
        "outputId": "59c5a0e3-5277-4216-e15c-224f6f31fd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training OUTDIR: /content/drive/MyDrive/ph-mckeanvlasov-diff/src/mckean-vlasov/runs/mv_sde/20250819_163913\n",
            "DATA_PT: /content/drive/MyDrive/ph-mckeanvlasov-diff/datasets/unified_topological_data_v6_semifast.pt\n",
            "Launching:\n",
            " /usr/bin/python3 -u main.py --data_pt /content/drive/MyDrive/ph-mckeanvlasov-diff/datasets/unified_topological_data_v6_semifast.pt --batch 2 --steps 20000 --seed 0 --lr 2e-4 --lr_energy 1e-3 --lr_enc 1e-3 --T 1000 --schedule cosine --v_pred --ema_decay 0.999 --use_energy --energy_scale 0.5 --energy_tau 0.07 --energy_gp 1e-4 --mf_mode rbf --mf_lambda 0.05 --mf_bandwidth 0.5 --outdir /content/drive/MyDrive/ph-mckeanvlasov-diff/src/mckean-vlasov/runs/mv_sde/20250819_163913 --ckpt_every 2000 --sample_every 2000 --sample_steps 250 --sample_label -1 --cfg_drop 0.1 --cfg_scale 3.0 --cfg_sched cosine --cfg_strength 5.0\n",
            "N=1000  vol=(N,H,W,KS,C)=(1000, 128, 128, 3, 3)  KS=3  degrees=3  res=128\n",
            "N=1000  vol=(N,H,W,KS,C)=(1000, 128, 128, 3, 3)  KS=3  degrees=3  res=128\n",
            "N=900  vol=(N,H,W,KS,C)=(900, 128, 128, 3, 3)\n"
          ]
        }
      ],
      "source": [
        "import shlex, datetime\n",
        "\n",
        "# === Set paths/args ===\n",
        "# If the auto search in previous cell didn’t find your dataset, set it explicitly here:\n",
        "DATA_PT = CANDIDATES[0] if len(CANDIDATES) else \"/content/drive/MyDrive/ph-mckeanvlasov-diff/datasets/unified_topological_data_v6_semifast.pt\"\n",
        "\n",
        "# Choose an output folder on Drive\n",
        "stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUTDIR = Path(\"/content/drive/MyDrive/ph-mckeanvlasov-diff/src/mckean-vlasov/runs/mv_sde\") / stamp\n",
        "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training args — tune as you like\n",
        "args = {\n",
        "    \"--data_pt\": DATA_PT,\n",
        "    \"--batch\": \"2\",                # bump to 24/32 on A100 if it fits\n",
        "    \"--steps\": \"20000\",\n",
        "    \"--seed\": \"0\",\n",
        "    \"--lr\": \"2e-4\",\n",
        "    \"--lr_energy\": \"1e-3\",\n",
        "    \"--lr_enc\": \"1e-3\",\n",
        "    \"--T\": \"1000\",\n",
        "    \"--schedule\": \"cosine\",\n",
        "    \"--v_pred\": \"\",                 # flag\n",
        "    \"--ema_decay\": \"0.999\",\n",
        "    \"--use_energy\": \"\",             # flag\n",
        "    \"--energy_scale\": \"0.5\",\n",
        "    \"--energy_tau\": \"0.07\",\n",
        "    \"--energy_gp\": \"1e-4\",\n",
        "    \"--mf_mode\": \"rbf\",\n",
        "    \"--mf_lambda\": \"0.05\",\n",
        "    \"--mf_bandwidth\": \"0.5\",\n",
        "    \"--outdir\": str(OUTDIR),\n",
        "    \"--ckpt_every\": \"2000\",\n",
        "    \"--sample_every\": \"2000\",\n",
        "    \"--sample_steps\": \"250\",\n",
        "    \"--sample_label\": \"-1\",\n",
        "    # If you want classifier-free guidance enabled in your main.py, add:\n",
        "    \"--cfg_drop\": \"0.1\",\n",
        "    \"--cfg_scale\": \"3.0\",\n",
        "    \"--cfg_sched\": \"cosine\",\n",
        "    \"--cfg_strength\": \"5.0\",\n",
        "}\n",
        "\n",
        "# Pretty print\n",
        "print(\"Training OUTDIR:\", OUTDIR)\n",
        "print(\"DATA_PT:\", DATA_PT)\n",
        "\n",
        "# Build argv\n",
        "argv = [sys.executable, \"-u\", \"main.py\"]\n",
        "for k, v in args.items():\n",
        "    if v == \"\":                       # boolean flags\n",
        "        argv.append(k)\n",
        "    else:\n",
        "        argv.extend([k, v])\n",
        "\n",
        "print('Launching:\\n', ' '.join(shlex.quote(a) for a in argv))\n",
        "\n",
        "# Stream logs live\n",
        "proc = subprocess.Popen(argv, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "try:\n",
        "    for line in proc.stdout:\n",
        "        print(line, end=\"\")\n",
        "finally:\n",
        "    ret = proc.wait()\n",
        "    print(\"\\nTraining exit code:\", ret)\n",
        "    if ret != 0:\n",
        "        raise SystemExit(ret)\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kMAF25cKe-T"
      },
      "id": "6kMAF25cKe-T",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}